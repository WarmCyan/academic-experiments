{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation in Feed Forward Neural Networks - Research Workbook\n",
    "\n",
    "Academic Exercise 0  \n",
    "Started 1/5/2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "### Artificial Neural Network\n",
    "\n",
    "<p><a href='https://en.wikipedia.org/wiki/Artificial_neural_network'>wikipedia:Artificial_neural_network</a></p>\n",
    "<p>A neural network (one of the basic concept of connectionism) is a computational attempt to mimic the way the biological brain works based on the concept of a network of connected neurons.</p>\n",
    "<p>The basic unit, the <b>neuron</b>, takes in inputs from previous \"layers\", sums them up, feeds that number through some <b>activation function</b>, then sends the result forward down to any other neurons it's connected to. (Referred to as <b>\"feed forward\"</b>)</p>\n",
    "<p>These networks are typically constructed in \"layers\". a row of neurons that each feed into each neuron in the next layer. The first layer is the input layer, where the problem inputs are supplied. The output layer is the one that everything eventually ends up feeding into, where the result is. Every layer in between is referred to as a <b>\"hidden layer\"</b></p>\n",
    "<p>The key part of a network is that every connection has an associated <b>weight</b>. Every number passed down through it from the originating neuron is multiplied by this weight before supplied to the target neuron.</p>\n",
    "</br>\n",
    "\n",
    "<p>The network as a whole essentially acts as a \"formula finder\". The goal is to get a series of weight multiplications/activations that produce a desired set of outputs given a set of inputs. The learning portion of this network works by attempting to manipulate the weight values to get closer and closer to a working solution.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feed-Forward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notation\n",
    "\n",
    "(Reference: <a href=\"http://neuralnetworksanddeeplearning.com/chap2.html\">neuralnetworksanddeeplearning.com</a>)\n",
    "\n",
    "$w^l_{jk}$ - weight for connection from $k$th neuron in in the $(l-1)$th layer to the $j$th neuron in the $l$th layer.   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ex: $w^3_{24}$ means the weight from the 4th node in the 2nd layer to the 2nd node in the 3rd layer\n",
    "\n",
    "$w^l$ - weight matrix for layer $l$. This consists of the weights connecting to the $l$th layer of neurons, \"that is, the entry in the $j$th row and $k$th column is $w^l_{jk}$\n",
    "\n",
    "$b^l$ - the bias vector for layer $l$\n",
    "\n",
    "$\\sigma$ - the sigmoid function: \n",
    "$$\n",
    "\\sigma(t) = \\frac{1}{1+e^{-t}}\n",
    "$$\n",
    "(so used as the activation function because it has a nice easy derivative!)\n",
    "\n",
    "$a^l$ - activation vector for layer $l$ (the activation value from each neuron).\n",
    "\n",
    "$a^l_j$ - activation for the $j$th node in layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Construction\n",
    "\n",
    "NOTE: all of the example code will be assuming a network structure of 1 hidden layer with 3 nodes, an input layer of 2 nodes, and an output layer of 1 node. (Attempting to emulate an XOR gate.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations\n",
    "\n",
    "Activations: An activation is calculated by first summing up all of the weights * previous layer's activations, (plus the bias weights) and then running this through the activation function. (In this case, we're using sigmoid)\n",
    "\n",
    "For an individual activation, this can be written:\n",
    "$$\n",
    "a^l_j = \\sigma(\\sum_k{w^l_{jk}a^{l-1}_{k}+b^l_j})\n",
    "$$\n",
    "Stepping through each part of the summation: $w^l_{jk}*a^{l-1}_k$ is the weights leading into this layer multipled with the activations (outputs) of the previous layer. Then to each individual value of this vector, you add the associated bias weights: $+b^l_j$\n",
    "\n",
    "This can also be written in a more compact matrix/vectorized form:\n",
    "$$\n",
    "a^l = \\sigma(w^la^{l-1}+b^l)\n",
    "$$\n",
    "This equates to the same as above, only done via vectors. $w^la^{l-1}$ Is multiplying the weights going into layer $l$ with the activations of the previous layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This contains the set of weights from layer i to layer i+1\n",
    "layers = []\n",
    "\n",
    "# structure\n",
    "inputSize = 2\n",
    "hiddenSize = 3\n",
    "outputSize = 1\n",
    "\n",
    "# initialize layer weights by structure\n",
    "\n",
    "for i in range(0, inputSize):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
